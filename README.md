# Airflow_flow
Learning Airflow


Spark 

We need Hadoop Cluster to Run Spark -- Seattle Data Guy 

Python Dag 

We Can Do fine with just local server 

Data Ingestion - Spark ( Pyspark ) 
Data Processing (ETL)- Pyspark Or PySpark in DataBricks Or Hive 
Data Orchestration - Airflow Or DataBricks Or Autosys 
Data Report - Tableau

Spark 

We need Hadoop Cluster to Run Spark -- Seattle Data Guy 

We Can Do fine with just local server 
Data Ingestion - Spark ( Pyspark ) 
Data Processing (ETL)- Pyspark Or PySpark in DataBricks Or Hive 
Data Orchestration - Airflow Or DataBricks Or Autosys 
Data Report - Tableau


Design Data Pipeline : Transfer Data from Producer to Consumer 

1) ETL 
2) ELT 


- Data Source : Database, Csv, Json files 
- ETL : Extract, Transform, Load ( Pyspark, Pandas, SQL's, Hive etc)
- Data Target/ Warehouse : Transformed Data Gets Stored 
- Data report : Reporting Tool on Top of Transformed Data


Data Ingestion : Unchanged Data Insert from Raw to Target ** better to take as Last 

* Airflow with pyspark seems to be Bad Idea, But Airflow with Pandas looks good -- Deep Dive 

Airflow is an Orchestrater - What Alternate incase of Pyspark ? DataBricks, Autosys etc 


